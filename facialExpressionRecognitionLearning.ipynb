{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "facialExpressionRecognitionLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNax/IhtPy/9fzqCq2bpYyF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatihcelikeee/FirstMachineLearningCodes/blob/master/facialExpressionRecognitionLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyglhoh4gofH"
      },
      "source": [
        "# Facial Expression Recognition Learning\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "* **Language:** Python \n",
        "\n",
        "* **Libaries:** Keras/TensorFlow\n",
        "\n",
        "* **Cloud:** Google Cloud, Ã¼cretsiz Colab servisi (Tesla K80 GPU)\n",
        "\n",
        "* **Dataset:** [KAGGLE- Challenges in Representation Learning: Facial Expression Recognition Challenge](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)\n",
        "\n",
        "**Extra resources:** \n",
        "\n",
        "1. [Pre-Processing](https://github.com/Hanzhuo/Facial-Expression-Recognition-with-TensorFlow-Convolutional-Neural-Networks/blob/master/CNN_Facial_Expression_Recogonition.ipynb)\n",
        "2. [Train and Model](https://github.com/piyush2896/Facial-Expression-Recognition-Challenge/blob/master/Facial-Expression-Recognition-Challenge.ipynb)\n",
        "3. [Visualization and additional definitions](https://github.com/sachin-kmr/ML-Facial-Expression-Recognition/blob/master/exp_recognition.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ROXUi1igwR"
      },
      "source": [
        "##Importing Libaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kki9Mjlgg3Zp",
        "outputId": "92a8e180-f5db-4dae-dce9-12480bacf1e9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "650Q6_19ho1t"
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir('/gdrive/My Drive/workSpace/facialExpressionRecognitionLearning')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eq-j6V0Fh7tI",
        "outputId": "d9c9e271-3284-4a84-9a6e-21c903f9f081"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "challenges-in-representation-learning-facial-expression-recognition-challenge\n",
            "facialExpressionRecognitionLearning.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zRvomhmiJ6u"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential, Model, model_from_json\n",
        "from keras.layers import Dense, Conv2D, Activation, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI2OgmzNir2w"
      },
      "source": [
        "##Reading Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L586-vyYifmj",
        "outputId": "e1a046fa-7c31-4041-a057-2b16a84b20ff"
      },
      "source": [
        "root = \"/gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/\"\n",
        "data = pd.read_csv(root + \"challenges-in-representation-learning-facial-expression-recognition-challenge/icml_face_data.csv\")\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35887, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "W7zd-tU7jtRL",
        "outputId": "864a62e0-afa4-47ea-dd89-6fe1ab288b62"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>Usage</th>\n",
              "      <th>pixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Training</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Training</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Training</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Training</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>Training</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   emotion     Usage                                             pixels\n",
              "0        0  Training  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
              "1        0  Training  151 150 147 155 148 133 111 140 170 174 182 15...\n",
              "2        2  Training  231 212 156 164 174 138 161 173 182 200 106 38...\n",
              "3        4  Training  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
              "4        6  Training  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghsxm7CDsdSu"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1AtQlK2o19U"
      },
      "source": [
        "np.unique(data[\" Usage\"].values.ravel())\n",
        "trainData = data[data[\" Usage\"] == \"Training\"]\n",
        "trainPixels = trainData[\" pixels\"].str.split(\" \").tolist() \n",
        "trainPixels = pd.DataFrame(trainPixels, dtype=int)\n",
        "trainImages = trainPixels.values\n",
        "trainImages = trainImages.astype(np.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "yoxLmYxApRhp",
        "outputId": "1c8f99f5-faa1-4dd4-95e1-7bb4d38cff1e"
      },
      "source": [
        "plt.imshow(trainImages[1905].reshape(48,48),cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6e0a8178d0>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXxb8rLir1x4",
        "outputId": "d96470b3-7cd9-4fc4-ae6e-f14d061d5195"
      },
      "source": [
        "trainLabelsFlat=trainData[\"emotion\"].values.ravel()\n",
        "trainLabelsCount = np.unique(trainLabelsFlat).shape[0]\n",
        "print('Number of different facial expressions: %d'%trainLabelsCount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of different facial expressions: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO8kibWbunaP",
        "outputId": "89a5b6ce-e7f5-4711-ad6f-76f39846e8af"
      },
      "source": [
        "import tensorflow as tf\n",
        "yTrain = tf.keras.utils.to_categorical(trainData[\"emotion\"])\n",
        "yTrain = yTrain.astype(np.uint8)\n",
        "\n",
        "print(yTrain.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28709, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjywG2KdwuIv",
        "outputId": "18285f29-bc62-4bc8-d6ea-e091e522aa82"
      },
      "source": [
        "np.unique(data[\" Usage\"].values.ravel()) \n",
        "\n",
        "print('Number of samples in the test dataset: %d'%(len(data[data[\" Usage\"] == \"PublicTest\"])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the test dataset: 3589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_xDvEkr6ZD9"
      },
      "source": [
        "testData = data[data[\" Usage\"] == \"PublicTest\"]\n",
        "testPixels = testData[\" pixels\"].str.split(\" \").tolist() \n",
        "\n",
        "testPixels = pd.DataFrame(testPixels, dtype=int)\n",
        "testImages = testPixels.values\n",
        "testImages = testImages.astype(np.float)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "PgBTGvLa7IaY",
        "outputId": "599d6447-7eeb-47f9-f85d-f11e17610014"
      },
      "source": [
        "plt.imshow(testImages[1905].reshape(48,48),cmap=\"gray\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6f09767750>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE_rKGxU7UWB",
        "outputId": "bb9d86b8-ba0d-4b39-e13e-b166cda3ee4d"
      },
      "source": [
        "yTest = tf.keras.utils.to_categorical(testData[\"emotion\"])\n",
        "yTest = yTest.astype(np.uint8)\n",
        "\n",
        "print(yTest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3589, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9bc0V0R84yT"
      },
      "source": [
        "##Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX3q9TSr7jTZ",
        "outputId": "e470dd20-0623-4d7e-e15c-fe4769f76681"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "#First Layer\n",
        "model.add(Conv2D(64, 3, data_format=\"channels_last\", kernel_initializer=\"he_normal\", input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "#Second Layer\n",
        "model.add(Conv2D(64, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides = 2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#3. Layer\n",
        "model.add(Conv2D(64, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "#4. Layer\n",
        "model.add(Conv2D(64, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "\n",
        "#5. Layer\n",
        "model.add(Conv2D(32, 3))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2), strides = 2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#FULL CONNECTION LAYER\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "#Last Layer\n",
        "model.add(Dense(7))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 46, 46, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 46, 46, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 46, 46, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 44, 44, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 44, 44, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 44, 44, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 22, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 20, 20, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 20, 20, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 20, 20, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 18, 18, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 18, 18, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 18, 18, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 16, 16, 32)        18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               262272    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 7)                 903       \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 394,727\n",
            "Trainable params: 393,895\n",
            "Non-trainable params: 832\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abn_ebqbAeHw",
        "outputId": "509ffe03-d8e7-4a90-8c99-65ccaa791071"
      },
      "source": [
        "xTrain = trainImages.reshape(-1,48,48,1)\n",
        "xTest = testImages.reshape(-1,48,48,1)\n",
        "print(xTrain.shape)\n",
        "print(xTest.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28709, 48, 48, 1)\n",
            "(3589, 48, 48, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90ZWv3QNBQt4",
        "outputId": "efd329a1-7e32-4308-ffd3-839fa7c433a3"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath=root + \"challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\", verbose = 1, save_best_only= True)\n",
        "epochs = 10\n",
        "batchSize = 100\n",
        "\n",
        "hist = model.fit(xTrain,yTrain,\n",
        "                 epochs=epochs,\n",
        "                 shuffle=True,\n",
        "                 batch_size = batchSize,\n",
        "                 validation_data = (xTest,yTest),\n",
        "                 callbacks=[checkpointer],\n",
        "                 verbose = 2)\n",
        "modelJson = model.to_json()\n",
        "with open (root+\"challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\", \"w+\") as jsonFile:\n",
        "  jsonFile.write(modelJson)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "288/288 - 51s - loss: 1.8570 - accuracy: 0.2933 - val_loss: 1.5000 - val_accuracy: 0.4179\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.50000, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\n",
            "Epoch 2/10\n",
            "288/288 - 19s - loss: 1.5357 - accuracy: 0.4035 - val_loss: 1.3714 - val_accuracy: 0.4609\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.50000 to 1.37144, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\n",
            "Epoch 3/10\n",
            "288/288 - 18s - loss: 1.4091 - accuracy: 0.4542 - val_loss: 1.3278 - val_accuracy: 0.4937\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.37144 to 1.32777, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\n",
            "Epoch 4/10\n",
            "288/288 - 18s - loss: 1.3408 - accuracy: 0.4853 - val_loss: 1.3527 - val_accuracy: 0.4695\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.32777\n",
            "Epoch 5/10\n",
            "288/288 - 18s - loss: 1.2945 - accuracy: 0.5043 - val_loss: 1.2502 - val_accuracy: 0.5196\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.32777 to 1.25017, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\n",
            "Epoch 6/10\n",
            "288/288 - 19s - loss: 1.2511 - accuracy: 0.5223 - val_loss: 1.2098 - val_accuracy: 0.5261\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.25017 to 1.20985, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\n",
            "Epoch 7/10\n",
            "288/288 - 19s - loss: 1.2269 - accuracy: 0.5317 - val_loss: 1.1945 - val_accuracy: 0.5386\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.20985 to 1.19453, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\n",
            "Epoch 8/10\n",
            "288/288 - 19s - loss: 1.1959 - accuracy: 0.5454 - val_loss: 1.1491 - val_accuracy: 0.5587\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.19453 to 1.14914, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\n",
            "Epoch 9/10\n",
            "288/288 - 19s - loss: 1.1748 - accuracy: 0.5563 - val_loss: 1.2225 - val_accuracy: 0.5316\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.14914\n",
            "Epoch 10/10\n",
            "288/288 - 19s - loss: 1.1504 - accuracy: 0.5630 - val_loss: 1.1124 - val_accuracy: 0.5729\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.14914 to 1.11239, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_k4gXoPgCvKH",
        "outputId": "f6c7e045-28cb-4e81-98d7-d6bb8928c139"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath=root + \"challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv2.h5\", verbose = 1, save_best_only= True)\n",
        "epochs = 10\n",
        "batchSize = 100\n",
        "\n",
        "hist = model.fit(xTrain,yTrain,\n",
        "                 epochs=epochs,\n",
        "                 shuffle=True,\n",
        "                 batch_size = batchSize,\n",
        "                 validation_data = (xTest,yTest),\n",
        "                 callbacks=[checkpointer],\n",
        "                 verbose = 2)\n",
        "modelJson = model.to_json()\n",
        "with open (root+\"challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\", \"w+\") as jsonFile:\n",
        "  jsonFile.write(modelJson)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "288/288 - 19s - loss: 1.1318 - accuracy: 0.5716 - val_loss: 1.1093 - val_accuracy: 0.5754\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.10934, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv2.h5\n",
            "Epoch 2/10\n",
            "288/288 - 19s - loss: 1.1208 - accuracy: 0.5742 - val_loss: 1.0978 - val_accuracy: 0.5851\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.10934 to 1.09782, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv2.h5\n",
            "Epoch 3/10\n",
            "288/288 - 19s - loss: 1.1036 - accuracy: 0.5802 - val_loss: 1.0898 - val_accuracy: 0.5862\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.09782 to 1.08975, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv2.h5\n",
            "Epoch 4/10\n",
            "288/288 - 19s - loss: 1.0861 - accuracy: 0.5876 - val_loss: 1.1012 - val_accuracy: 0.5843\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.08975\n",
            "Epoch 5/10\n",
            "288/288 - 19s - loss: 1.0782 - accuracy: 0.5915 - val_loss: 1.1055 - val_accuracy: 0.5809\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.08975\n",
            "Epoch 6/10\n",
            "288/288 - 19s - loss: 1.0646 - accuracy: 0.5967 - val_loss: 1.1018 - val_accuracy: 0.5876\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.08975\n",
            "Epoch 7/10\n",
            "288/288 - 19s - loss: 1.0515 - accuracy: 0.6048 - val_loss: 1.1081 - val_accuracy: 0.5868\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.08975\n",
            "Epoch 8/10\n",
            "288/288 - 19s - loss: 1.0444 - accuracy: 0.6090 - val_loss: 1.0538 - val_accuracy: 0.6007\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.08975 to 1.05378, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv2.h5\n",
            "Epoch 9/10\n",
            "288/288 - 19s - loss: 1.0234 - accuracy: 0.6109 - val_loss: 1.0700 - val_accuracy: 0.5991\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.05378\n",
            "Epoch 10/10\n",
            "288/288 - 19s - loss: 1.0147 - accuracy: 0.6170 - val_loss: 1.1252 - val_accuracy: 0.5773\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.05378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veIjhv-qD9Oh",
        "outputId": "39d758cc-75e8-46e8-be49-7c294af9a14a"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath=root + \"challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv3.h5\", verbose = 1, save_best_only= True)\n",
        "epochs = 300\n",
        "batchSize = 100\n",
        "\n",
        "hist = model.fit(xTrain,yTrain,\n",
        "                 epochs=epochs,\n",
        "                 shuffle=True,\n",
        "                 batch_size = batchSize,\n",
        "                 validation_data = (xTest,yTest),\n",
        "                 callbacks=[checkpointer],\n",
        "                 verbose = 2)\n",
        "modelJson = model.to_json()\n",
        "with open (root+\"challenges-in-representation-learning-facial-expression-recognition-challenge/faceModel.h5\", \"w+\") as jsonFile:\n",
        "  jsonFile.write(modelJson)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "288/288 - 19s - loss: 1.0018 - accuracy: 0.6182 - val_loss: 1.0752 - val_accuracy: 0.5993\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.07516, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv3.h5\n",
            "Epoch 2/300\n",
            "288/288 - 19s - loss: 0.9997 - accuracy: 0.6259 - val_loss: 1.0452 - val_accuracy: 0.6055\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.07516 to 1.04521, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv3.h5\n",
            "Epoch 3/300\n",
            "288/288 - 19s - loss: 0.9912 - accuracy: 0.6272 - val_loss: 1.0573 - val_accuracy: 0.5954\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.04521\n",
            "Epoch 4/300\n",
            "288/288 - 19s - loss: 0.9812 - accuracy: 0.6308 - val_loss: 1.0225 - val_accuracy: 0.6166\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.04521 to 1.02253, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv3.h5\n",
            "Epoch 5/300\n",
            "288/288 - 19s - loss: 0.9647 - accuracy: 0.6362 - val_loss: 1.0356 - val_accuracy: 0.6074\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 1.02253\n",
            "Epoch 6/300\n",
            "288/288 - 19s - loss: 0.9609 - accuracy: 0.6360 - val_loss: 1.0432 - val_accuracy: 0.6094\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.02253\n",
            "Epoch 7/300\n",
            "288/288 - 19s - loss: 0.9486 - accuracy: 0.6436 - val_loss: 1.0280 - val_accuracy: 0.6069\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.02253\n",
            "Epoch 8/300\n",
            "288/288 - 19s - loss: 0.9449 - accuracy: 0.6445 - val_loss: 1.0303 - val_accuracy: 0.6180\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.02253\n",
            "Epoch 9/300\n",
            "288/288 - 19s - loss: 0.9372 - accuracy: 0.6458 - val_loss: 1.0180 - val_accuracy: 0.6077\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.02253 to 1.01803, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv3.h5\n",
            "Epoch 10/300\n",
            "288/288 - 19s - loss: 0.9307 - accuracy: 0.6479 - val_loss: 1.0213 - val_accuracy: 0.6147\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.01803\n",
            "Epoch 11/300\n",
            "288/288 - 19s - loss: 0.9237 - accuracy: 0.6527 - val_loss: 1.0290 - val_accuracy: 0.6085\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.01803\n",
            "Epoch 12/300\n",
            "288/288 - 19s - loss: 0.9192 - accuracy: 0.6561 - val_loss: 1.0296 - val_accuracy: 0.6149\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.01803\n",
            "Epoch 13/300\n",
            "288/288 - 18s - loss: 0.9095 - accuracy: 0.6564 - val_loss: 1.0390 - val_accuracy: 0.6160\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.01803\n",
            "Epoch 14/300\n",
            "288/288 - 19s - loss: 0.9023 - accuracy: 0.6549 - val_loss: 1.0427 - val_accuracy: 0.6155\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.01803\n",
            "Epoch 15/300\n",
            "288/288 - 19s - loss: 0.8941 - accuracy: 0.6603 - val_loss: 1.0297 - val_accuracy: 0.6183\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.01803\n",
            "Epoch 16/300\n",
            "288/288 - 19s - loss: 0.8954 - accuracy: 0.6617 - val_loss: 1.0344 - val_accuracy: 0.6074\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.01803\n",
            "Epoch 17/300\n",
            "288/288 - 19s - loss: 0.8889 - accuracy: 0.6632 - val_loss: 1.0253 - val_accuracy: 0.6191\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.01803\n",
            "Epoch 18/300\n",
            "288/288 - 19s - loss: 0.8841 - accuracy: 0.6671 - val_loss: 1.0376 - val_accuracy: 0.6183\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.01803\n",
            "Epoch 19/300\n",
            "288/288 - 19s - loss: 0.8837 - accuracy: 0.6643 - val_loss: 1.0414 - val_accuracy: 0.6160\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.01803\n",
            "Epoch 20/300\n",
            "288/288 - 19s - loss: 0.8739 - accuracy: 0.6702 - val_loss: 1.0240 - val_accuracy: 0.6208\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.01803\n",
            "Epoch 21/300\n",
            "288/288 - 19s - loss: 0.8626 - accuracy: 0.6733 - val_loss: 1.0373 - val_accuracy: 0.6138\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.01803\n",
            "Epoch 22/300\n",
            "288/288 - 19s - loss: 0.8597 - accuracy: 0.6770 - val_loss: 0.9950 - val_accuracy: 0.6308\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.01803 to 0.99503, saving model to /gdrive/My Drive/workSpace/facialExpressionRecognitionLearning/challenges-in-representation-learning-facial-expression-recognition-challenge/faceModelv3.h5\n",
            "Epoch 23/300\n",
            "288/288 - 19s - loss: 0.8655 - accuracy: 0.6737 - val_loss: 1.0078 - val_accuracy: 0.6286\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.99503\n",
            "Epoch 24/300\n",
            "288/288 - 19s - loss: 0.8572 - accuracy: 0.6777 - val_loss: 1.0106 - val_accuracy: 0.6258\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.99503\n",
            "Epoch 25/300\n",
            "288/288 - 19s - loss: 0.8492 - accuracy: 0.6776 - val_loss: 1.0187 - val_accuracy: 0.6230\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.99503\n",
            "Epoch 26/300\n",
            "288/288 - 19s - loss: 0.8480 - accuracy: 0.6786 - val_loss: 1.0390 - val_accuracy: 0.6241\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.99503\n",
            "Epoch 27/300\n",
            "288/288 - 19s - loss: 0.8368 - accuracy: 0.6852 - val_loss: 1.0406 - val_accuracy: 0.6208\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.99503\n",
            "Epoch 28/300\n",
            "288/288 - 19s - loss: 0.8281 - accuracy: 0.6880 - val_loss: 1.0345 - val_accuracy: 0.6269\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.99503\n",
            "Epoch 29/300\n",
            "288/288 - 19s - loss: 0.8399 - accuracy: 0.6774 - val_loss: 1.0131 - val_accuracy: 0.6289\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.99503\n",
            "Epoch 30/300\n",
            "288/288 - 19s - loss: 0.8285 - accuracy: 0.6853 - val_loss: 1.0174 - val_accuracy: 0.6283\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.99503\n",
            "Epoch 31/300\n",
            "288/288 - 19s - loss: 0.8213 - accuracy: 0.6879 - val_loss: 1.0131 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.99503\n",
            "Epoch 32/300\n",
            "288/288 - 19s - loss: 0.8235 - accuracy: 0.6891 - val_loss: 1.0552 - val_accuracy: 0.6172\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.99503\n",
            "Epoch 33/300\n",
            "288/288 - 19s - loss: 0.8073 - accuracy: 0.6920 - val_loss: 1.0466 - val_accuracy: 0.6202\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.99503\n",
            "Epoch 34/300\n",
            "288/288 - 19s - loss: 0.8128 - accuracy: 0.6902 - val_loss: 1.0384 - val_accuracy: 0.6269\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.99503\n",
            "Epoch 35/300\n",
            "288/288 - 19s - loss: 0.8119 - accuracy: 0.6941 - val_loss: 1.0466 - val_accuracy: 0.6317\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.99503\n",
            "Epoch 36/300\n",
            "288/288 - 19s - loss: 0.8112 - accuracy: 0.6924 - val_loss: 1.0445 - val_accuracy: 0.6269\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.99503\n",
            "Epoch 37/300\n",
            "288/288 - 19s - loss: 0.7936 - accuracy: 0.6987 - val_loss: 1.0648 - val_accuracy: 0.6258\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.99503\n",
            "Epoch 38/300\n",
            "288/288 - 19s - loss: 0.7967 - accuracy: 0.6969 - val_loss: 1.0109 - val_accuracy: 0.6403\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.99503\n",
            "Epoch 39/300\n",
            "288/288 - 19s - loss: 0.7988 - accuracy: 0.6963 - val_loss: 1.0237 - val_accuracy: 0.6252\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.99503\n",
            "Epoch 40/300\n",
            "288/288 - 19s - loss: 0.7889 - accuracy: 0.7013 - val_loss: 1.0294 - val_accuracy: 0.6322\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.99503\n",
            "Epoch 41/300\n",
            "288/288 - 18s - loss: 0.7958 - accuracy: 0.6996 - val_loss: 1.0683 - val_accuracy: 0.6174\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.99503\n",
            "Epoch 42/300\n",
            "288/288 - 18s - loss: 0.7821 - accuracy: 0.7026 - val_loss: 1.0158 - val_accuracy: 0.6266\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.99503\n",
            "Epoch 43/300\n",
            "288/288 - 18s - loss: 0.7827 - accuracy: 0.7044 - val_loss: 1.0379 - val_accuracy: 0.6280\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.99503\n",
            "Epoch 44/300\n",
            "288/288 - 18s - loss: 0.7814 - accuracy: 0.7017 - val_loss: 1.0332 - val_accuracy: 0.6239\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.99503\n",
            "Epoch 45/300\n",
            "288/288 - 18s - loss: 0.7738 - accuracy: 0.7087 - val_loss: 1.0225 - val_accuracy: 0.6250\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.99503\n",
            "Epoch 46/300\n",
            "288/288 - 18s - loss: 0.7706 - accuracy: 0.7098 - val_loss: 1.0189 - val_accuracy: 0.6356\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.99503\n",
            "Epoch 47/300\n",
            "288/288 - 19s - loss: 0.7749 - accuracy: 0.7051 - val_loss: 1.0453 - val_accuracy: 0.6252\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.99503\n",
            "Epoch 48/300\n",
            "288/288 - 19s - loss: 0.7729 - accuracy: 0.7075 - val_loss: 1.0624 - val_accuracy: 0.6213\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.99503\n",
            "Epoch 49/300\n",
            "288/288 - 19s - loss: 0.7811 - accuracy: 0.7050 - val_loss: 1.0184 - val_accuracy: 0.6322\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.99503\n",
            "Epoch 50/300\n",
            "288/288 - 19s - loss: 0.7678 - accuracy: 0.7064 - val_loss: 1.0421 - val_accuracy: 0.6258\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.99503\n",
            "Epoch 51/300\n",
            "288/288 - 19s - loss: 0.7623 - accuracy: 0.7103 - val_loss: 1.0831 - val_accuracy: 0.6261\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.99503\n",
            "Epoch 52/300\n",
            "288/288 - 19s - loss: 0.7475 - accuracy: 0.7165 - val_loss: 1.0294 - val_accuracy: 0.6347\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.99503\n",
            "Epoch 53/300\n",
            "288/288 - 18s - loss: 0.7561 - accuracy: 0.7154 - val_loss: 1.0178 - val_accuracy: 0.6317\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.99503\n",
            "Epoch 54/300\n",
            "288/288 - 19s - loss: 0.7584 - accuracy: 0.7135 - val_loss: 1.0434 - val_accuracy: 0.6264\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.99503\n",
            "Epoch 55/300\n",
            "288/288 - 19s - loss: 0.7542 - accuracy: 0.7152 - val_loss: 1.0118 - val_accuracy: 0.6353\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.99503\n",
            "Epoch 56/300\n",
            "288/288 - 19s - loss: 0.7540 - accuracy: 0.7136 - val_loss: 1.0306 - val_accuracy: 0.6283\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.99503\n",
            "Epoch 57/300\n",
            "288/288 - 18s - loss: 0.7474 - accuracy: 0.7183 - val_loss: 1.0297 - val_accuracy: 0.6322\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.99503\n",
            "Epoch 58/300\n",
            "288/288 - 18s - loss: 0.7419 - accuracy: 0.7201 - val_loss: 1.0120 - val_accuracy: 0.6369\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.99503\n",
            "Epoch 59/300\n",
            "288/288 - 18s - loss: 0.7419 - accuracy: 0.7195 - val_loss: 1.0319 - val_accuracy: 0.6353\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.99503\n",
            "Epoch 60/300\n",
            "288/288 - 18s - loss: 0.7451 - accuracy: 0.7179 - val_loss: 1.0274 - val_accuracy: 0.6325\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.99503\n",
            "Epoch 61/300\n",
            "288/288 - 18s - loss: 0.7442 - accuracy: 0.7208 - val_loss: 1.0434 - val_accuracy: 0.6305\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.99503\n",
            "Epoch 62/300\n",
            "288/288 - 18s - loss: 0.7377 - accuracy: 0.7182 - val_loss: 1.0336 - val_accuracy: 0.6364\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.99503\n",
            "Epoch 63/300\n",
            "288/288 - 18s - loss: 0.7333 - accuracy: 0.7222 - val_loss: 1.0264 - val_accuracy: 0.6414\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.99503\n",
            "Epoch 64/300\n",
            "288/288 - 18s - loss: 0.7312 - accuracy: 0.7218 - val_loss: 1.0314 - val_accuracy: 0.6353\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.99503\n",
            "Epoch 65/300\n",
            "288/288 - 18s - loss: 0.7303 - accuracy: 0.7232 - val_loss: 1.0029 - val_accuracy: 0.6431\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.99503\n",
            "Epoch 66/300\n",
            "288/288 - 18s - loss: 0.7210 - accuracy: 0.7252 - val_loss: 1.0265 - val_accuracy: 0.6442\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.99503\n",
            "Epoch 67/300\n",
            "288/288 - 18s - loss: 0.7219 - accuracy: 0.7252 - val_loss: 1.0320 - val_accuracy: 0.6400\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.99503\n",
            "Epoch 68/300\n",
            "288/288 - 18s - loss: 0.7285 - accuracy: 0.7239 - val_loss: 1.0373 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.99503\n",
            "Epoch 69/300\n",
            "288/288 - 18s - loss: 0.7245 - accuracy: 0.7267 - val_loss: 1.0342 - val_accuracy: 0.6395\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.99503\n",
            "Epoch 70/300\n",
            "288/288 - 18s - loss: 0.7223 - accuracy: 0.7253 - val_loss: 1.0392 - val_accuracy: 0.6414\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.99503\n",
            "Epoch 71/300\n",
            "288/288 - 18s - loss: 0.7222 - accuracy: 0.7281 - val_loss: 1.0624 - val_accuracy: 0.6325\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.99503\n",
            "Epoch 72/300\n",
            "288/288 - 18s - loss: 0.7159 - accuracy: 0.7311 - val_loss: 1.0217 - val_accuracy: 0.6461\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.99503\n",
            "Epoch 73/300\n",
            "288/288 - 18s - loss: 0.7150 - accuracy: 0.7281 - val_loss: 1.0491 - val_accuracy: 0.6322\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.99503\n",
            "Epoch 74/300\n",
            "288/288 - 18s - loss: 0.7052 - accuracy: 0.7335 - val_loss: 1.0455 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.99503\n",
            "Epoch 75/300\n",
            "288/288 - 18s - loss: 0.7053 - accuracy: 0.7356 - val_loss: 1.0145 - val_accuracy: 0.6439\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.99503\n",
            "Epoch 76/300\n",
            "288/288 - 18s - loss: 0.7118 - accuracy: 0.7295 - val_loss: 1.0427 - val_accuracy: 0.6358\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.99503\n",
            "Epoch 77/300\n",
            "288/288 - 18s - loss: 0.7157 - accuracy: 0.7298 - val_loss: 1.0782 - val_accuracy: 0.6205\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.99503\n",
            "Epoch 78/300\n",
            "288/288 - 18s - loss: 0.7106 - accuracy: 0.7317 - val_loss: 1.0626 - val_accuracy: 0.6314\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.99503\n",
            "Epoch 79/300\n",
            "288/288 - 18s - loss: 0.7061 - accuracy: 0.7333 - val_loss: 1.0612 - val_accuracy: 0.6358\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.99503\n",
            "Epoch 80/300\n",
            "288/288 - 18s - loss: 0.7070 - accuracy: 0.7320 - val_loss: 1.0265 - val_accuracy: 0.6436\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.99503\n",
            "Epoch 81/300\n",
            "288/288 - 18s - loss: 0.7052 - accuracy: 0.7312 - val_loss: 1.0426 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.99503\n",
            "Epoch 82/300\n",
            "288/288 - 18s - loss: 0.6994 - accuracy: 0.7351 - val_loss: 1.0483 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.99503\n",
            "Epoch 83/300\n",
            "288/288 - 18s - loss: 0.6981 - accuracy: 0.7383 - val_loss: 1.0438 - val_accuracy: 0.6328\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.99503\n",
            "Epoch 84/300\n",
            "288/288 - 18s - loss: 0.6965 - accuracy: 0.7364 - val_loss: 1.0276 - val_accuracy: 0.6397\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.99503\n",
            "Epoch 85/300\n",
            "288/288 - 18s - loss: 0.6984 - accuracy: 0.7318 - val_loss: 1.0240 - val_accuracy: 0.6456\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.99503\n",
            "Epoch 86/300\n",
            "288/288 - 18s - loss: 0.6989 - accuracy: 0.7352 - val_loss: 1.0312 - val_accuracy: 0.6353\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.99503\n",
            "Epoch 87/300\n",
            "288/288 - 18s - loss: 0.6951 - accuracy: 0.7359 - val_loss: 1.0320 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.99503\n",
            "Epoch 88/300\n",
            "288/288 - 18s - loss: 0.6923 - accuracy: 0.7412 - val_loss: 1.0349 - val_accuracy: 0.6461\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.99503\n",
            "Epoch 89/300\n",
            "288/288 - 18s - loss: 0.6910 - accuracy: 0.7402 - val_loss: 1.0323 - val_accuracy: 0.6347\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.99503\n",
            "Epoch 90/300\n",
            "288/288 - 18s - loss: 0.6997 - accuracy: 0.7359 - val_loss: 1.0520 - val_accuracy: 0.6450\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.99503\n",
            "Epoch 91/300\n",
            "288/288 - 18s - loss: 0.6869 - accuracy: 0.7420 - val_loss: 1.0477 - val_accuracy: 0.6397\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.99503\n",
            "Epoch 92/300\n",
            "288/288 - 18s - loss: 0.6882 - accuracy: 0.7395 - val_loss: 1.0482 - val_accuracy: 0.6481\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.99503\n",
            "Epoch 93/300\n",
            "288/288 - 18s - loss: 0.6884 - accuracy: 0.7414 - val_loss: 1.0768 - val_accuracy: 0.6403\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.99503\n",
            "Epoch 94/300\n",
            "288/288 - 18s - loss: 0.6807 - accuracy: 0.7426 - val_loss: 1.0328 - val_accuracy: 0.6464\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.99503\n",
            "Epoch 95/300\n",
            "288/288 - 18s - loss: 0.6852 - accuracy: 0.7403 - val_loss: 1.0564 - val_accuracy: 0.6319\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.99503\n",
            "Epoch 96/300\n",
            "288/288 - 18s - loss: 0.6779 - accuracy: 0.7447 - val_loss: 1.0510 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.99503\n",
            "Epoch 97/300\n",
            "288/288 - 18s - loss: 0.6851 - accuracy: 0.7398 - val_loss: 1.0687 - val_accuracy: 0.6367\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.99503\n",
            "Epoch 98/300\n",
            "288/288 - 18s - loss: 0.6766 - accuracy: 0.7443 - val_loss: 1.0469 - val_accuracy: 0.6478\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.99503\n",
            "Epoch 99/300\n",
            "288/288 - 18s - loss: 0.6684 - accuracy: 0.7469 - val_loss: 1.0518 - val_accuracy: 0.6447\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.99503\n",
            "Epoch 100/300\n",
            "288/288 - 18s - loss: 0.6715 - accuracy: 0.7449 - val_loss: 1.0449 - val_accuracy: 0.6411\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.99503\n",
            "Epoch 101/300\n",
            "288/288 - 18s - loss: 0.6797 - accuracy: 0.7423 - val_loss: 1.0368 - val_accuracy: 0.6445\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.99503\n",
            "Epoch 102/300\n",
            "288/288 - 18s - loss: 0.6753 - accuracy: 0.7480 - val_loss: 1.0297 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.99503\n",
            "Epoch 103/300\n",
            "288/288 - 18s - loss: 0.6761 - accuracy: 0.7436 - val_loss: 1.0589 - val_accuracy: 0.6372\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.99503\n",
            "Epoch 104/300\n",
            "288/288 - 18s - loss: 0.6766 - accuracy: 0.7425 - val_loss: 1.0730 - val_accuracy: 0.6350\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.99503\n",
            "Epoch 105/300\n",
            "288/288 - 19s - loss: 0.6665 - accuracy: 0.7467 - val_loss: 1.0888 - val_accuracy: 0.6406\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.99503\n",
            "Epoch 106/300\n",
            "288/288 - 19s - loss: 0.6748 - accuracy: 0.7471 - val_loss: 1.0261 - val_accuracy: 0.6481\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.99503\n",
            "Epoch 107/300\n",
            "288/288 - 19s - loss: 0.6655 - accuracy: 0.7485 - val_loss: 1.0595 - val_accuracy: 0.6397\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.99503\n",
            "Epoch 108/300\n",
            "288/288 - 19s - loss: 0.6743 - accuracy: 0.7444 - val_loss: 1.0433 - val_accuracy: 0.6361\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.99503\n",
            "Epoch 109/300\n",
            "288/288 - 19s - loss: 0.6696 - accuracy: 0.7467 - val_loss: 1.0357 - val_accuracy: 0.6422\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.99503\n",
            "Epoch 110/300\n",
            "288/288 - 18s - loss: 0.6641 - accuracy: 0.7493 - val_loss: 1.0932 - val_accuracy: 0.6339\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.99503\n",
            "Epoch 111/300\n",
            "288/288 - 19s - loss: 0.6638 - accuracy: 0.7499 - val_loss: 1.0524 - val_accuracy: 0.6392\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.99503\n",
            "Epoch 112/300\n",
            "288/288 - 19s - loss: 0.6628 - accuracy: 0.7469 - val_loss: 1.0493 - val_accuracy: 0.6350\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.99503\n",
            "Epoch 113/300\n",
            "288/288 - 19s - loss: 0.6624 - accuracy: 0.7504 - val_loss: 1.0405 - val_accuracy: 0.6486\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.99503\n",
            "Epoch 114/300\n",
            "288/288 - 19s - loss: 0.6613 - accuracy: 0.7485 - val_loss: 1.0451 - val_accuracy: 0.6375\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.99503\n",
            "Epoch 115/300\n",
            "288/288 - 19s - loss: 0.6638 - accuracy: 0.7474 - val_loss: 1.0372 - val_accuracy: 0.6439\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.99503\n",
            "Epoch 116/300\n",
            "288/288 - 19s - loss: 0.6499 - accuracy: 0.7538 - val_loss: 1.0660 - val_accuracy: 0.6456\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.99503\n",
            "Epoch 117/300\n",
            "288/288 - 19s - loss: 0.6548 - accuracy: 0.7534 - val_loss: 1.0680 - val_accuracy: 0.6434\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.99503\n",
            "Epoch 118/300\n",
            "288/288 - 19s - loss: 0.6534 - accuracy: 0.7520 - val_loss: 1.0643 - val_accuracy: 0.6453\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.99503\n",
            "Epoch 119/300\n",
            "288/288 - 19s - loss: 0.6586 - accuracy: 0.7474 - val_loss: 1.0567 - val_accuracy: 0.6314\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.99503\n",
            "Epoch 120/300\n",
            "288/288 - 19s - loss: 0.6520 - accuracy: 0.7521 - val_loss: 1.0520 - val_accuracy: 0.6470\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.99503\n",
            "Epoch 121/300\n",
            "288/288 - 19s - loss: 0.6496 - accuracy: 0.7552 - val_loss: 1.0587 - val_accuracy: 0.6459\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.99503\n",
            "Epoch 122/300\n",
            "288/288 - 19s - loss: 0.6542 - accuracy: 0.7516 - val_loss: 1.0661 - val_accuracy: 0.6442\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.99503\n",
            "Epoch 123/300\n",
            "288/288 - 19s - loss: 0.6481 - accuracy: 0.7545 - val_loss: 1.0515 - val_accuracy: 0.6498\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.99503\n",
            "Epoch 124/300\n",
            "288/288 - 19s - loss: 0.6418 - accuracy: 0.7590 - val_loss: 1.0432 - val_accuracy: 0.6445\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.99503\n",
            "Epoch 125/300\n",
            "288/288 - 19s - loss: 0.6520 - accuracy: 0.7558 - val_loss: 1.0434 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.99503\n",
            "Epoch 126/300\n",
            "288/288 - 19s - loss: 0.6498 - accuracy: 0.7520 - val_loss: 1.0661 - val_accuracy: 0.6461\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.99503\n",
            "Epoch 127/300\n",
            "288/288 - 19s - loss: 0.6434 - accuracy: 0.7566 - val_loss: 1.0607 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.99503\n",
            "Epoch 128/300\n",
            "288/288 - 19s - loss: 0.6505 - accuracy: 0.7540 - val_loss: 1.0658 - val_accuracy: 0.6406\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.99503\n",
            "Epoch 129/300\n",
            "288/288 - 19s - loss: 0.6421 - accuracy: 0.7557 - val_loss: 1.0514 - val_accuracy: 0.6481\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.99503\n",
            "Epoch 130/300\n",
            "288/288 - 19s - loss: 0.6426 - accuracy: 0.7592 - val_loss: 1.0814 - val_accuracy: 0.6442\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.99503\n",
            "Epoch 131/300\n",
            "288/288 - 19s - loss: 0.6425 - accuracy: 0.7594 - val_loss: 1.0745 - val_accuracy: 0.6439\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.99503\n",
            "Epoch 132/300\n",
            "288/288 - 19s - loss: 0.6399 - accuracy: 0.7528 - val_loss: 1.0521 - val_accuracy: 0.6439\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.99503\n",
            "Epoch 133/300\n",
            "288/288 - 19s - loss: 0.6430 - accuracy: 0.7567 - val_loss: 1.0790 - val_accuracy: 0.6422\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.99503\n",
            "Epoch 134/300\n",
            "288/288 - 19s - loss: 0.6384 - accuracy: 0.7577 - val_loss: 1.0568 - val_accuracy: 0.6436\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.99503\n",
            "Epoch 135/300\n",
            "288/288 - 19s - loss: 0.6329 - accuracy: 0.7590 - val_loss: 1.0564 - val_accuracy: 0.6434\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.99503\n",
            "Epoch 136/300\n",
            "288/288 - 19s - loss: 0.6417 - accuracy: 0.7594 - val_loss: 1.0537 - val_accuracy: 0.6509\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.99503\n",
            "Epoch 137/300\n",
            "288/288 - 19s - loss: 0.6421 - accuracy: 0.7591 - val_loss: 1.0753 - val_accuracy: 0.6420\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.99503\n",
            "Epoch 138/300\n",
            "288/288 - 19s - loss: 0.6371 - accuracy: 0.7596 - val_loss: 1.0829 - val_accuracy: 0.6442\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.99503\n",
            "Epoch 139/300\n",
            "288/288 - 19s - loss: 0.6404 - accuracy: 0.7581 - val_loss: 1.0720 - val_accuracy: 0.6428\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.99503\n",
            "Epoch 140/300\n",
            "288/288 - 19s - loss: 0.6371 - accuracy: 0.7600 - val_loss: 1.0671 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.99503\n",
            "Epoch 141/300\n",
            "288/288 - 19s - loss: 0.6360 - accuracy: 0.7580 - val_loss: 1.0572 - val_accuracy: 0.6436\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.99503\n",
            "Epoch 142/300\n",
            "288/288 - 19s - loss: 0.6346 - accuracy: 0.7621 - val_loss: 1.0815 - val_accuracy: 0.6481\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.99503\n",
            "Epoch 143/300\n",
            "288/288 - 19s - loss: 0.6341 - accuracy: 0.7618 - val_loss: 1.0783 - val_accuracy: 0.6383\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.99503\n",
            "Epoch 144/300\n",
            "288/288 - 19s - loss: 0.6289 - accuracy: 0.7601 - val_loss: 1.0898 - val_accuracy: 0.6397\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.99503\n",
            "Epoch 145/300\n",
            "288/288 - 19s - loss: 0.6212 - accuracy: 0.7634 - val_loss: 1.0691 - val_accuracy: 0.6408\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.99503\n",
            "Epoch 146/300\n",
            "288/288 - 19s - loss: 0.6285 - accuracy: 0.7600 - val_loss: 1.0646 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.99503\n",
            "Epoch 147/300\n",
            "288/288 - 18s - loss: 0.6260 - accuracy: 0.7645 - val_loss: 1.0767 - val_accuracy: 0.6417\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.99503\n",
            "Epoch 148/300\n",
            "288/288 - 19s - loss: 0.6281 - accuracy: 0.7645 - val_loss: 1.0748 - val_accuracy: 0.6514\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.99503\n",
            "Epoch 149/300\n",
            "288/288 - 19s - loss: 0.6258 - accuracy: 0.7610 - val_loss: 1.0785 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.99503\n",
            "Epoch 150/300\n",
            "288/288 - 19s - loss: 0.6193 - accuracy: 0.7638 - val_loss: 1.0609 - val_accuracy: 0.6400\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.99503\n",
            "Epoch 151/300\n",
            "288/288 - 19s - loss: 0.6200 - accuracy: 0.7659 - val_loss: 1.0728 - val_accuracy: 0.6378\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.99503\n",
            "Epoch 152/300\n",
            "288/288 - 19s - loss: 0.6235 - accuracy: 0.7615 - val_loss: 1.0993 - val_accuracy: 0.6425\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.99503\n",
            "Epoch 153/300\n",
            "288/288 - 18s - loss: 0.6256 - accuracy: 0.7644 - val_loss: 1.1070 - val_accuracy: 0.6339\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.99503\n",
            "Epoch 154/300\n",
            "288/288 - 18s - loss: 0.6227 - accuracy: 0.7643 - val_loss: 1.0862 - val_accuracy: 0.6439\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.99503\n",
            "Epoch 155/300\n",
            "288/288 - 18s - loss: 0.6244 - accuracy: 0.7651 - val_loss: 1.1091 - val_accuracy: 0.6411\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.99503\n",
            "Epoch 156/300\n",
            "288/288 - 18s - loss: 0.6264 - accuracy: 0.7642 - val_loss: 1.0699 - val_accuracy: 0.6514\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.99503\n",
            "Epoch 157/300\n",
            "288/288 - 18s - loss: 0.6139 - accuracy: 0.7686 - val_loss: 1.0823 - val_accuracy: 0.6439\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.99503\n",
            "Epoch 158/300\n",
            "288/288 - 18s - loss: 0.6240 - accuracy: 0.7623 - val_loss: 1.0850 - val_accuracy: 0.6425\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.99503\n",
            "Epoch 159/300\n",
            "288/288 - 18s - loss: 0.6176 - accuracy: 0.7663 - val_loss: 1.0813 - val_accuracy: 0.6486\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.99503\n",
            "Epoch 160/300\n",
            "288/288 - 18s - loss: 0.6134 - accuracy: 0.7663 - val_loss: 1.0763 - val_accuracy: 0.6456\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.99503\n",
            "Epoch 161/300\n",
            "288/288 - 18s - loss: 0.6207 - accuracy: 0.7660 - val_loss: 1.0859 - val_accuracy: 0.6336\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.99503\n",
            "Epoch 162/300\n",
            "288/288 - 18s - loss: 0.6160 - accuracy: 0.7673 - val_loss: 1.0909 - val_accuracy: 0.6470\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.99503\n",
            "Epoch 163/300\n",
            "288/288 - 18s - loss: 0.6128 - accuracy: 0.7701 - val_loss: 1.0967 - val_accuracy: 0.6381\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.99503\n",
            "Epoch 164/300\n",
            "288/288 - 18s - loss: 0.6199 - accuracy: 0.7676 - val_loss: 1.0537 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.99503\n",
            "Epoch 165/300\n",
            "288/288 - 18s - loss: 0.6130 - accuracy: 0.7660 - val_loss: 1.0876 - val_accuracy: 0.6386\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.99503\n",
            "Epoch 166/300\n",
            "288/288 - 18s - loss: 0.6059 - accuracy: 0.7710 - val_loss: 1.0934 - val_accuracy: 0.6406\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.99503\n",
            "Epoch 167/300\n",
            "288/288 - 19s - loss: 0.6122 - accuracy: 0.7685 - val_loss: 1.1067 - val_accuracy: 0.6450\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.99503\n",
            "Epoch 168/300\n",
            "288/288 - 19s - loss: 0.6062 - accuracy: 0.7719 - val_loss: 1.0978 - val_accuracy: 0.6406\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.99503\n",
            "Epoch 169/300\n",
            "288/288 - 18s - loss: 0.6120 - accuracy: 0.7689 - val_loss: 1.0871 - val_accuracy: 0.6486\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.99503\n",
            "Epoch 170/300\n",
            "288/288 - 18s - loss: 0.6115 - accuracy: 0.7729 - val_loss: 1.0937 - val_accuracy: 0.6436\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.99503\n",
            "Epoch 171/300\n",
            "288/288 - 18s - loss: 0.6149 - accuracy: 0.7677 - val_loss: 1.1180 - val_accuracy: 0.6425\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.99503\n",
            "Epoch 172/300\n",
            "288/288 - 18s - loss: 0.6105 - accuracy: 0.7687 - val_loss: 1.0745 - val_accuracy: 0.6528\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.99503\n",
            "Epoch 173/300\n",
            "288/288 - 18s - loss: 0.6017 - accuracy: 0.7738 - val_loss: 1.0655 - val_accuracy: 0.6478\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.99503\n",
            "Epoch 174/300\n",
            "288/288 - 18s - loss: 0.6055 - accuracy: 0.7724 - val_loss: 1.0831 - val_accuracy: 0.6434\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.99503\n",
            "Epoch 175/300\n",
            "288/288 - 18s - loss: 0.6070 - accuracy: 0.7723 - val_loss: 1.0750 - val_accuracy: 0.6431\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.99503\n",
            "Epoch 176/300\n",
            "288/288 - 18s - loss: 0.6109 - accuracy: 0.7672 - val_loss: 1.0819 - val_accuracy: 0.6473\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.99503\n",
            "Epoch 177/300\n",
            "288/288 - 18s - loss: 0.6075 - accuracy: 0.7751 - val_loss: 1.0802 - val_accuracy: 0.6484\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.99503\n",
            "Epoch 178/300\n",
            "288/288 - 18s - loss: 0.6120 - accuracy: 0.7706 - val_loss: 1.0643 - val_accuracy: 0.6473\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.99503\n",
            "Epoch 179/300\n",
            "288/288 - 18s - loss: 0.6037 - accuracy: 0.7725 - val_loss: 1.0538 - val_accuracy: 0.6500\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.99503\n",
            "Epoch 180/300\n",
            "288/288 - 18s - loss: 0.6078 - accuracy: 0.7745 - val_loss: 1.0561 - val_accuracy: 0.6439\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.99503\n",
            "Epoch 181/300\n",
            "288/288 - 18s - loss: 0.6064 - accuracy: 0.7743 - val_loss: 1.0843 - val_accuracy: 0.6425\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.99503\n",
            "Epoch 182/300\n",
            "288/288 - 18s - loss: 0.6019 - accuracy: 0.7743 - val_loss: 1.0617 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.99503\n",
            "Epoch 183/300\n",
            "288/288 - 18s - loss: 0.6070 - accuracy: 0.7699 - val_loss: 1.0614 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.99503\n",
            "Epoch 184/300\n",
            "288/288 - 18s - loss: 0.6022 - accuracy: 0.7696 - val_loss: 1.0747 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.99503\n",
            "Epoch 185/300\n",
            "288/288 - 18s - loss: 0.6032 - accuracy: 0.7746 - val_loss: 1.1041 - val_accuracy: 0.6420\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.99503\n",
            "Epoch 186/300\n",
            "288/288 - 18s - loss: 0.5947 - accuracy: 0.7755 - val_loss: 1.0573 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.99503\n",
            "Epoch 187/300\n",
            "288/288 - 18s - loss: 0.5938 - accuracy: 0.7755 - val_loss: 1.0707 - val_accuracy: 0.6439\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.99503\n",
            "Epoch 188/300\n",
            "288/288 - 18s - loss: 0.6001 - accuracy: 0.7733 - val_loss: 1.1058 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.99503\n",
            "Epoch 189/300\n",
            "288/288 - 18s - loss: 0.6050 - accuracy: 0.7702 - val_loss: 1.1056 - val_accuracy: 0.6447\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.99503\n",
            "Epoch 190/300\n",
            "288/288 - 18s - loss: 0.5938 - accuracy: 0.7776 - val_loss: 1.0873 - val_accuracy: 0.6517\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.99503\n",
            "Epoch 191/300\n",
            "288/288 - 18s - loss: 0.6010 - accuracy: 0.7744 - val_loss: 1.0701 - val_accuracy: 0.6534\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.99503\n",
            "Epoch 192/300\n",
            "288/288 - 18s - loss: 0.5893 - accuracy: 0.7786 - val_loss: 1.1054 - val_accuracy: 0.6500\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.99503\n",
            "Epoch 193/300\n",
            "288/288 - 18s - loss: 0.6025 - accuracy: 0.7750 - val_loss: 1.0632 - val_accuracy: 0.6562\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.99503\n",
            "Epoch 194/300\n",
            "288/288 - 18s - loss: 0.5919 - accuracy: 0.7754 - val_loss: 1.0632 - val_accuracy: 0.6570\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.99503\n",
            "Epoch 195/300\n",
            "288/288 - 18s - loss: 0.5968 - accuracy: 0.7732 - val_loss: 1.0681 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.99503\n",
            "Epoch 196/300\n",
            "288/288 - 18s - loss: 0.5832 - accuracy: 0.7809 - val_loss: 1.1134 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.99503\n",
            "Epoch 197/300\n",
            "288/288 - 18s - loss: 0.5963 - accuracy: 0.7747 - val_loss: 1.1165 - val_accuracy: 0.6503\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.99503\n",
            "Epoch 198/300\n",
            "288/288 - 18s - loss: 0.5970 - accuracy: 0.7762 - val_loss: 1.1161 - val_accuracy: 0.6417\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.99503\n",
            "Epoch 199/300\n",
            "288/288 - 18s - loss: 0.5954 - accuracy: 0.7764 - val_loss: 1.0992 - val_accuracy: 0.6461\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.99503\n",
            "Epoch 200/300\n",
            "288/288 - 18s - loss: 0.5840 - accuracy: 0.7802 - val_loss: 1.0874 - val_accuracy: 0.6406\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.99503\n",
            "Epoch 201/300\n",
            "288/288 - 18s - loss: 0.5956 - accuracy: 0.7733 - val_loss: 1.0944 - val_accuracy: 0.6486\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.99503\n",
            "Epoch 202/300\n",
            "288/288 - 18s - loss: 0.5957 - accuracy: 0.7757 - val_loss: 1.1019 - val_accuracy: 0.6503\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.99503\n",
            "Epoch 203/300\n",
            "288/288 - 18s - loss: 0.5992 - accuracy: 0.7739 - val_loss: 1.1210 - val_accuracy: 0.6428\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.99503\n",
            "Epoch 204/300\n",
            "288/288 - 18s - loss: 0.5850 - accuracy: 0.7778 - val_loss: 1.1025 - val_accuracy: 0.6478\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.99503\n",
            "Epoch 205/300\n",
            "288/288 - 18s - loss: 0.5941 - accuracy: 0.7790 - val_loss: 1.0919 - val_accuracy: 0.6523\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.99503\n",
            "Epoch 206/300\n",
            "288/288 - 18s - loss: 0.5977 - accuracy: 0.7726 - val_loss: 1.0749 - val_accuracy: 0.6447\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.99503\n",
            "Epoch 207/300\n",
            "288/288 - 18s - loss: 0.5903 - accuracy: 0.7794 - val_loss: 1.0715 - val_accuracy: 0.6492\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.99503\n",
            "Epoch 208/300\n",
            "288/288 - 18s - loss: 0.5990 - accuracy: 0.7739 - val_loss: 1.0920 - val_accuracy: 0.6461\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.99503\n",
            "Epoch 209/300\n",
            "288/288 - 18s - loss: 0.5925 - accuracy: 0.7767 - val_loss: 1.0866 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.99503\n",
            "Epoch 210/300\n",
            "288/288 - 18s - loss: 0.5920 - accuracy: 0.7749 - val_loss: 1.0652 - val_accuracy: 0.6383\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.99503\n",
            "Epoch 211/300\n",
            "288/288 - 18s - loss: 0.5859 - accuracy: 0.7779 - val_loss: 1.1083 - val_accuracy: 0.6445\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.99503\n",
            "Epoch 212/300\n",
            "288/288 - 18s - loss: 0.5854 - accuracy: 0.7814 - val_loss: 1.1076 - val_accuracy: 0.6456\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.99503\n",
            "Epoch 213/300\n",
            "288/288 - 18s - loss: 0.5834 - accuracy: 0.7832 - val_loss: 1.0788 - val_accuracy: 0.6414\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.99503\n",
            "Epoch 214/300\n",
            "288/288 - 18s - loss: 0.5847 - accuracy: 0.7792 - val_loss: 1.0706 - val_accuracy: 0.6573\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.99503\n",
            "Epoch 215/300\n",
            "288/288 - 18s - loss: 0.5719 - accuracy: 0.7852 - val_loss: 1.0981 - val_accuracy: 0.6506\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.99503\n",
            "Epoch 216/300\n",
            "288/288 - 19s - loss: 0.5853 - accuracy: 0.7828 - val_loss: 1.1109 - val_accuracy: 0.6506\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.99503\n",
            "Epoch 217/300\n",
            "288/288 - 19s - loss: 0.5776 - accuracy: 0.7819 - val_loss: 1.0891 - val_accuracy: 0.6481\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.99503\n",
            "Epoch 218/300\n",
            "288/288 - 19s - loss: 0.5895 - accuracy: 0.7794 - val_loss: 1.0619 - val_accuracy: 0.6459\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.99503\n",
            "Epoch 219/300\n",
            "288/288 - 19s - loss: 0.5818 - accuracy: 0.7800 - val_loss: 1.1036 - val_accuracy: 0.6456\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.99503\n",
            "Epoch 220/300\n",
            "288/288 - 19s - loss: 0.5861 - accuracy: 0.7792 - val_loss: 1.0898 - val_accuracy: 0.6486\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.99503\n",
            "Epoch 221/300\n",
            "288/288 - 19s - loss: 0.5802 - accuracy: 0.7822 - val_loss: 1.0855 - val_accuracy: 0.6447\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.99503\n",
            "Epoch 222/300\n",
            "288/288 - 19s - loss: 0.5810 - accuracy: 0.7819 - val_loss: 1.0719 - val_accuracy: 0.6400\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.99503\n",
            "Epoch 223/300\n",
            "288/288 - 19s - loss: 0.5771 - accuracy: 0.7852 - val_loss: 1.0839 - val_accuracy: 0.6425\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.99503\n",
            "Epoch 224/300\n",
            "288/288 - 19s - loss: 0.5856 - accuracy: 0.7792 - val_loss: 1.1677 - val_accuracy: 0.6300\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.99503\n",
            "Epoch 225/300\n",
            "288/288 - 19s - loss: 0.5776 - accuracy: 0.7840 - val_loss: 1.1406 - val_accuracy: 0.6389\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.99503\n",
            "Epoch 226/300\n",
            "288/288 - 19s - loss: 0.5725 - accuracy: 0.7857 - val_loss: 1.1058 - val_accuracy: 0.6450\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.99503\n",
            "Epoch 227/300\n",
            "288/288 - 19s - loss: 0.5739 - accuracy: 0.7848 - val_loss: 1.0709 - val_accuracy: 0.6492\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.99503\n",
            "Epoch 228/300\n",
            "288/288 - 19s - loss: 0.5745 - accuracy: 0.7833 - val_loss: 1.0958 - val_accuracy: 0.6422\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.99503\n",
            "Epoch 229/300\n",
            "288/288 - 19s - loss: 0.5818 - accuracy: 0.7837 - val_loss: 1.1086 - val_accuracy: 0.6406\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.99503\n",
            "Epoch 230/300\n",
            "288/288 - 19s - loss: 0.5818 - accuracy: 0.7802 - val_loss: 1.0863 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.99503\n",
            "Epoch 231/300\n",
            "288/288 - 19s - loss: 0.5734 - accuracy: 0.7841 - val_loss: 1.1050 - val_accuracy: 0.6417\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.99503\n",
            "Epoch 232/300\n",
            "288/288 - 19s - loss: 0.5749 - accuracy: 0.7818 - val_loss: 1.0960 - val_accuracy: 0.6453\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.99503\n",
            "Epoch 233/300\n",
            "288/288 - 19s - loss: 0.5725 - accuracy: 0.7859 - val_loss: 1.1139 - val_accuracy: 0.6470\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.99503\n",
            "Epoch 234/300\n",
            "288/288 - 19s - loss: 0.5794 - accuracy: 0.7826 - val_loss: 1.1086 - val_accuracy: 0.6464\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.99503\n",
            "Epoch 235/300\n",
            "288/288 - 19s - loss: 0.5667 - accuracy: 0.7874 - val_loss: 1.0736 - val_accuracy: 0.6470\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.99503\n",
            "Epoch 236/300\n",
            "288/288 - 19s - loss: 0.5770 - accuracy: 0.7818 - val_loss: 1.0830 - val_accuracy: 0.6425\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.99503\n",
            "Epoch 237/300\n",
            "288/288 - 19s - loss: 0.5770 - accuracy: 0.7819 - val_loss: 1.1079 - val_accuracy: 0.6481\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.99503\n",
            "Epoch 238/300\n",
            "288/288 - 19s - loss: 0.5841 - accuracy: 0.7829 - val_loss: 1.0714 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.99503\n",
            "Epoch 239/300\n",
            "288/288 - 19s - loss: 0.5787 - accuracy: 0.7808 - val_loss: 1.0901 - val_accuracy: 0.6450\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.99503\n",
            "Epoch 240/300\n",
            "288/288 - 19s - loss: 0.5683 - accuracy: 0.7892 - val_loss: 1.0732 - val_accuracy: 0.6523\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.99503\n",
            "Epoch 241/300\n",
            "288/288 - 19s - loss: 0.5700 - accuracy: 0.7878 - val_loss: 1.1164 - val_accuracy: 0.6484\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.99503\n",
            "Epoch 242/300\n",
            "288/288 - 19s - loss: 0.5683 - accuracy: 0.7860 - val_loss: 1.0830 - val_accuracy: 0.6534\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.99503\n",
            "Epoch 243/300\n",
            "288/288 - 19s - loss: 0.5726 - accuracy: 0.7864 - val_loss: 1.0754 - val_accuracy: 0.6425\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.99503\n",
            "Epoch 244/300\n",
            "288/288 - 19s - loss: 0.5669 - accuracy: 0.7877 - val_loss: 1.0828 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.99503\n",
            "Epoch 245/300\n",
            "288/288 - 19s - loss: 0.5669 - accuracy: 0.7865 - val_loss: 1.0865 - val_accuracy: 0.6525\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.99503\n",
            "Epoch 246/300\n",
            "288/288 - 19s - loss: 0.5672 - accuracy: 0.7862 - val_loss: 1.1011 - val_accuracy: 0.6528\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.99503\n",
            "Epoch 247/300\n",
            "288/288 - 19s - loss: 0.5710 - accuracy: 0.7860 - val_loss: 1.1044 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.99503\n",
            "Epoch 248/300\n",
            "288/288 - 19s - loss: 0.5610 - accuracy: 0.7912 - val_loss: 1.0900 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.99503\n",
            "Epoch 249/300\n",
            "288/288 - 19s - loss: 0.5694 - accuracy: 0.7858 - val_loss: 1.0769 - val_accuracy: 0.6498\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.99503\n",
            "Epoch 250/300\n",
            "288/288 - 19s - loss: 0.5663 - accuracy: 0.7885 - val_loss: 1.0741 - val_accuracy: 0.6525\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.99503\n",
            "Epoch 251/300\n",
            "288/288 - 19s - loss: 0.5630 - accuracy: 0.7895 - val_loss: 1.0854 - val_accuracy: 0.6434\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.99503\n",
            "Epoch 252/300\n",
            "288/288 - 19s - loss: 0.5589 - accuracy: 0.7883 - val_loss: 1.0863 - val_accuracy: 0.6514\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.99503\n",
            "Epoch 253/300\n",
            "288/288 - 19s - loss: 0.5676 - accuracy: 0.7875 - val_loss: 1.0726 - val_accuracy: 0.6509\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.99503\n",
            "Epoch 254/300\n",
            "288/288 - 19s - loss: 0.5612 - accuracy: 0.7864 - val_loss: 1.0841 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.99503\n",
            "Epoch 255/300\n",
            "288/288 - 19s - loss: 0.5680 - accuracy: 0.7873 - val_loss: 1.0758 - val_accuracy: 0.6500\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.99503\n",
            "Epoch 256/300\n",
            "288/288 - 19s - loss: 0.5683 - accuracy: 0.7861 - val_loss: 1.1003 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.99503\n",
            "Epoch 257/300\n",
            "288/288 - 19s - loss: 0.5637 - accuracy: 0.7879 - val_loss: 1.0895 - val_accuracy: 0.6520\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.99503\n",
            "Epoch 258/300\n",
            "288/288 - 19s - loss: 0.5657 - accuracy: 0.7876 - val_loss: 1.0710 - val_accuracy: 0.6581\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.99503\n",
            "Epoch 259/300\n",
            "288/288 - 19s - loss: 0.5624 - accuracy: 0.7919 - val_loss: 1.1010 - val_accuracy: 0.6470\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.99503\n",
            "Epoch 260/300\n",
            "288/288 - 19s - loss: 0.5771 - accuracy: 0.7849 - val_loss: 1.1173 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.99503\n",
            "Epoch 261/300\n",
            "288/288 - 19s - loss: 0.5582 - accuracy: 0.7882 - val_loss: 1.0977 - val_accuracy: 0.6425\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.99503\n",
            "Epoch 262/300\n",
            "288/288 - 19s - loss: 0.5590 - accuracy: 0.7881 - val_loss: 1.0978 - val_accuracy: 0.6473\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.99503\n",
            "Epoch 263/300\n",
            "288/288 - 19s - loss: 0.5625 - accuracy: 0.7892 - val_loss: 1.1069 - val_accuracy: 0.6489\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.99503\n",
            "Epoch 264/300\n",
            "288/288 - 19s - loss: 0.5621 - accuracy: 0.7868 - val_loss: 1.1014 - val_accuracy: 0.6498\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.99503\n",
            "Epoch 265/300\n",
            "288/288 - 19s - loss: 0.5606 - accuracy: 0.7909 - val_loss: 1.1335 - val_accuracy: 0.6422\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.99503\n",
            "Epoch 266/300\n",
            "288/288 - 19s - loss: 0.5610 - accuracy: 0.7921 - val_loss: 1.0937 - val_accuracy: 0.6459\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.99503\n",
            "Epoch 267/300\n",
            "288/288 - 19s - loss: 0.5579 - accuracy: 0.7873 - val_loss: 1.0980 - val_accuracy: 0.6456\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.99503\n",
            "Epoch 268/300\n",
            "288/288 - 19s - loss: 0.5543 - accuracy: 0.7921 - val_loss: 1.1132 - val_accuracy: 0.6528\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.99503\n",
            "Epoch 269/300\n",
            "288/288 - 19s - loss: 0.5554 - accuracy: 0.7902 - val_loss: 1.0879 - val_accuracy: 0.6509\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.99503\n",
            "Epoch 270/300\n",
            "288/288 - 19s - loss: 0.5529 - accuracy: 0.7927 - val_loss: 1.1215 - val_accuracy: 0.6436\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.99503\n",
            "Epoch 271/300\n",
            "288/288 - 19s - loss: 0.5591 - accuracy: 0.7902 - val_loss: 1.1144 - val_accuracy: 0.6470\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.99503\n",
            "Epoch 272/300\n",
            "288/288 - 19s - loss: 0.5575 - accuracy: 0.7940 - val_loss: 1.1184 - val_accuracy: 0.6464\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.99503\n",
            "Epoch 273/300\n",
            "288/288 - 19s - loss: 0.5564 - accuracy: 0.7912 - val_loss: 1.1158 - val_accuracy: 0.6531\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.99503\n",
            "Epoch 274/300\n",
            "288/288 - 19s - loss: 0.5512 - accuracy: 0.7950 - val_loss: 1.0918 - val_accuracy: 0.6467\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.99503\n",
            "Epoch 275/300\n",
            "288/288 - 19s - loss: 0.5586 - accuracy: 0.7895 - val_loss: 1.1013 - val_accuracy: 0.6590\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.99503\n",
            "Epoch 276/300\n",
            "288/288 - 18s - loss: 0.5697 - accuracy: 0.7847 - val_loss: 1.1128 - val_accuracy: 0.6551\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.99503\n",
            "Epoch 277/300\n",
            "288/288 - 18s - loss: 0.5545 - accuracy: 0.7922 - val_loss: 1.0693 - val_accuracy: 0.6503\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.99503\n",
            "Epoch 278/300\n",
            "288/288 - 18s - loss: 0.5561 - accuracy: 0.7936 - val_loss: 1.1169 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.99503\n",
            "Epoch 279/300\n",
            "288/288 - 18s - loss: 0.5525 - accuracy: 0.7950 - val_loss: 1.0758 - val_accuracy: 0.6506\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.99503\n",
            "Epoch 280/300\n",
            "288/288 - 18s - loss: 0.5533 - accuracy: 0.7925 - val_loss: 1.1244 - val_accuracy: 0.6525\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.99503\n",
            "Epoch 281/300\n",
            "288/288 - 18s - loss: 0.5592 - accuracy: 0.7914 - val_loss: 1.1096 - val_accuracy: 0.6422\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.99503\n",
            "Epoch 282/300\n",
            "288/288 - 18s - loss: 0.5530 - accuracy: 0.7915 - val_loss: 1.1153 - val_accuracy: 0.6486\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.99503\n",
            "Epoch 283/300\n",
            "288/288 - 18s - loss: 0.5522 - accuracy: 0.7927 - val_loss: 1.0939 - val_accuracy: 0.6450\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.99503\n",
            "Epoch 284/300\n",
            "288/288 - 18s - loss: 0.5505 - accuracy: 0.7898 - val_loss: 1.1115 - val_accuracy: 0.6498\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.99503\n",
            "Epoch 285/300\n",
            "288/288 - 18s - loss: 0.5455 - accuracy: 0.7947 - val_loss: 1.0968 - val_accuracy: 0.6506\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.99503\n",
            "Epoch 286/300\n",
            "288/288 - 18s - loss: 0.5480 - accuracy: 0.7982 - val_loss: 1.1246 - val_accuracy: 0.6473\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.99503\n",
            "Epoch 287/300\n",
            "288/288 - 18s - loss: 0.5546 - accuracy: 0.7930 - val_loss: 1.1217 - val_accuracy: 0.6475\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.99503\n",
            "Epoch 288/300\n",
            "288/288 - 18s - loss: 0.5456 - accuracy: 0.7942 - val_loss: 1.1187 - val_accuracy: 0.6534\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.99503\n",
            "Epoch 289/300\n",
            "288/288 - 18s - loss: 0.5493 - accuracy: 0.7943 - val_loss: 1.1201 - val_accuracy: 0.6481\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.99503\n",
            "Epoch 290/300\n",
            "288/288 - 18s - loss: 0.5500 - accuracy: 0.7952 - val_loss: 1.1039 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.99503\n",
            "Epoch 291/300\n",
            "288/288 - 18s - loss: 0.5526 - accuracy: 0.7914 - val_loss: 1.1015 - val_accuracy: 0.6478\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.99503\n",
            "Epoch 292/300\n",
            "288/288 - 18s - loss: 0.5520 - accuracy: 0.7932 - val_loss: 1.1091 - val_accuracy: 0.6498\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.99503\n",
            "Epoch 293/300\n",
            "288/288 - 18s - loss: 0.5490 - accuracy: 0.7947 - val_loss: 1.0786 - val_accuracy: 0.6509\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.99503\n",
            "Epoch 294/300\n",
            "288/288 - 18s - loss: 0.5525 - accuracy: 0.7928 - val_loss: 1.0993 - val_accuracy: 0.6562\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.99503\n",
            "Epoch 295/300\n",
            "288/288 - 18s - loss: 0.5439 - accuracy: 0.7964 - val_loss: 1.1021 - val_accuracy: 0.6470\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.99503\n",
            "Epoch 296/300\n",
            "288/288 - 18s - loss: 0.5532 - accuracy: 0.7945 - val_loss: 1.1014 - val_accuracy: 0.6553\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.99503\n",
            "Epoch 297/300\n",
            "288/288 - 18s - loss: 0.5475 - accuracy: 0.7949 - val_loss: 1.1237 - val_accuracy: 0.6512\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.99503\n",
            "Epoch 298/300\n",
            "288/288 - 18s - loss: 0.5443 - accuracy: 0.7997 - val_loss: 1.0883 - val_accuracy: 0.6523\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.99503\n",
            "Epoch 299/300\n",
            "288/288 - 18s - loss: 0.5507 - accuracy: 0.7938 - val_loss: 1.1022 - val_accuracy: 0.6442\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.99503\n",
            "Epoch 300/300\n",
            "288/288 - 18s - loss: 0.5481 - accuracy: 0.7964 - val_loss: 1.0995 - val_accuracy: 0.6495\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.99503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj2edcAqEyBp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}